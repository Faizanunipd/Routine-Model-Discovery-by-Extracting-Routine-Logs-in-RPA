{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5112f1e-6978-40c7-8021-9ae27eb06b6c",
   "metadata": {},
   "source": [
    "## *import libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f357fd-8283-408a-bbb4-b0ac7bf28df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "import pm4py\n",
    "from pm4py.algo.evaluation import algorithm\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_replay\n",
    "from pm4py.objects.petri_net.importer.variants import pnml as pnml_importer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b28f6-5fa4-4999-8cd1-428843ecd260",
   "metadata": {},
   "source": [
    "## *Parameter Setting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34e3ed6-405e-4008-90c3-00557cc552f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    'encoding': 'freq',   # freq, dur\n",
    "    'export_net': False,    # True, False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89e422-7339-46b9-8a08-68553a8f4bed",
   "metadata": {},
   "source": [
    "## *Read Event Log*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c571ce-b177-447c-97de-c593a2fdd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UI_read_log(file_path, labels_path):\n",
    "    cluster_labels_df = pd.read_csv(label_file_path) \n",
    "    total_pred = cluster_labels_df.shape[0]\n",
    "    trace_id = np.arange(1, total_pred+1).tolist()\n",
    "    cluster_labels_df['case:concept:name'] = trace_id\n",
    "    cluster_labels_df['case:concept:name'] = cluster_labels_df['case:concept:name'].astype(str)\n",
    "\n",
    "    event_log = f.read_log(file_path)\n",
    "    event_log['time:timestamp'] = pd.to_datetime(event_log['time:timestamp'], errors='coerce')\n",
    "    event_log['time:timestamp'].ffill(inplace=True)\n",
    "\n",
    "    return event_log, cluster_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80d47e1-72b8-439e-9bfb-8e788d1becf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segments(event_log, cluster_labels_df):\n",
    "    # Given lists\n",
    "    seg_pred = list(cluster_labels_df['prediction'])\n",
    "    if all(val == -1 for val in seg_pred):\n",
    "        seg_pred = [0] * len(seg_pred)\n",
    "        # seg_pred = list(range(len(seg_pred)))\n",
    "\n",
    "    cluster_labels_df['prediction'] = seg_pred   \n",
    "    \n",
    "    bounds = list(cluster_labels_df['bound'])\n",
    "    # Initialize the third list with default values (-1)\n",
    "    max_bound = max(bounds)+1\n",
    "    segments = [-1] * max_bound  # Length of third list is max bound\n",
    "    \n",
    "    # Filling the third list based on bounds and corresponding labels\n",
    "    start_idx = 0\n",
    "    trace_id = 1\n",
    "    for i, bound in enumerate(bounds):\n",
    "        label = seg_pred[i]  # Get the corresponding label\n",
    "        segments[start_idx:bound+1] = [trace_id] * (bound+1 - start_idx)  # Assign label to the range\n",
    "        # third_list[start_idx:bound+1] = trace_id  # Assign label to the range\n",
    "        start_idx = bound+1  # Update the start index for the next range\n",
    "        trace_id = trace_id+1\n",
    "        \n",
    "    # Output the third list\n",
    "    # print(segments)\n",
    "    \n",
    "    event_log['case:concept:name'] = segments\n",
    "    event_log['case:concept:name'] = event_log['case:concept:name'].astype(str)\n",
    "    return event_log, cluster_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da87628-128f-4c69-96c0-f7f6615ccba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise_samples(encoded_log):\n",
    "    features_log = f.clustering_preprocessing(encoded_log)\n",
    "    if 'DBSCAN_Cluster' in encoded_log.columns:\n",
    "        cluster_column = 'DBSCAN_Cluster'\n",
    "    else:\n",
    "        cluster_column = 'cluster_label'\n",
    "\n",
    "    features_log[cluster_column] = encoded_log[cluster_column]\n",
    "    \n",
    "    # Find noise indices\n",
    "    noise_indices = encoded_log[encoded_log[cluster_column] == -1].index\n",
    "\n",
    "    if len(noise_indices) >= 1:\n",
    "    \n",
    "        # Find the nearest cluster for each noise point\n",
    "        nearest_cluster_indices, _ = pairwise_distances_argmin_min(\n",
    "            features_log.iloc[noise_indices, :-1],  # Exclude the cluster label column\n",
    "            features_log[features_log[cluster_column] != -1].iloc[:, :-1]  # Exclude noise points\n",
    "        )\n",
    "        \n",
    "        # Merge noise samples into the nearest cluster\n",
    "        encoded_log.loc[noise_indices, cluster_column] = encoded_log.loc[\n",
    "            encoded_log[cluster_column] != -1, cluster_column\n",
    "        ].iloc[nearest_cluster_indices].values\n",
    "        \n",
    "        # Check if there are still noise points\n",
    "        remaining_noise_indices = encoded_log[encoded_log[cluster_column] == -1].index\n",
    "        if len(remaining_noise_indices) > 0:\n",
    "            # print(f\"\\nThere are still {len(remaining_noise_indices)} noise points remaining.\\n\")\n",
    "            pass\n",
    "        else:\n",
    "            # print(\"\\nAll noise points have been assigned to the nearest cluster.\\n\")\n",
    "            pass\n",
    "\n",
    "        # print(\"\\n\", encoded_log[cluster_column].value_counts(), \"\\n\")\n",
    "    return encoded_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866a2917-8383-44ef-94a9-d1976d12c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acitvity_log(session_log, encoded_log):\n",
    "    cluster_column = 'cluster_label'\n",
    "    clusters = encoded_log[cluster_column].unique()\n",
    "    cluster_map = {c: f\"routine_{c+1}\" for c in clusters}\n",
    "    \n",
    "    merged_log = pd.merge(session_log, encoded_log, on=['case:concept:name', 'Session'], how='inner')\n",
    "    merged_log = merged_log[['case:concept:name', 'Session', 'time:timestamp', 'concept:name', cluster_column]]\n",
    "    \n",
    "    merged_log[cluster_column] = merged_log[cluster_column].replace(cluster_map)\n",
    "    return merged_log, cluster_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d1ffb-3e59-44ee-8555-767b6a1af8ff",
   "metadata": {},
   "source": [
    "## *Util Functions (Evaluation Scores)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17ab2ca-d54f-497a-a8c9-1bd345739ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transitions (activities) from both models to calculate Jaccard Coefficient\n",
    "def extract_transitions(net):\n",
    "    \"\"\" Extracts the transitions (activities) from the given Petri net model. \"\"\"\n",
    "    return {t.label for t in net.transitions if t.label is not None}\n",
    "\n",
    "\n",
    "def get_fScore(fitness, precision):\n",
    "    if fitness + precision == 0:\n",
    "        return 0\n",
    "    f_score = (2*fitness*precision)/(fitness+precision)\n",
    "    return f_score\n",
    "\n",
    "\n",
    "def get_JC(log, net, im, fm, routine_label):\n",
    "    gt_routine_activities = extract_transitions(net)\n",
    "    routine_activities = set(log['concept:name'])\n",
    "    # print(gt_routine_activities, \"\\n\")\n",
    "    # print(routine_activities, \"\\n\")\n",
    "    # print(\"=\"*30)\n",
    "\n",
    "    # Calculate the Jaccard Coefficient (Intersection over Union)\n",
    "    intersection = routine_activities.intersection(gt_routine_activities)\n",
    "    union = routine_activities.union(gt_routine_activities)\n",
    "    jaccard_coefficient = len(intersection) / len(union)\n",
    "\n",
    "    return jaccard_coefficient\n",
    "\n",
    "\n",
    "def token_base_evaluation(log, net, im, fm):\n",
    "    replayed_traces = token_replay.apply(log, net, im, fm)\n",
    "    \n",
    "    # Calculate Support (fraction of transitions supported by the log)\n",
    "    activated_transitions = set()\n",
    "    for trace in replayed_traces:\n",
    "        for trans in trace['activated_transitions']:\n",
    "            if trans is not None:  # Ensure the transition is valid\n",
    "                activated_transitions.add(trans)\n",
    "    total_transitions = len(net.transitions)\n",
    "    support = len(activated_transitions) / total_transitions\n",
    "\n",
    "    # Calculate Coverage\n",
    "    covered_traces = sum([1 for trace in replayed_traces if trace['activated_transitions']])\n",
    "    total_traces = len(log)\n",
    "    coverage = covered_traces / total_traces\n",
    "\n",
    "    return support, coverage\n",
    "    \n",
    "\n",
    "def alignment_base_evaluation(log, net, im, fm):\n",
    "    # Apply alignment-based conformance checking\n",
    "    aligned_traces = alignments.apply_log(log, net, im, fm)\n",
    "    \n",
    "    # Calculate Support (fraction of transitions supported by the log)\n",
    "    activated_transitions = set()\n",
    "    for trace in aligned_traces:\n",
    "        for step in trace['alignment']:\n",
    "            # Check if the step corresponds to a 'model move' (indicating a supported transition)\n",
    "            if step[0] == step[1] and step[1] is not None:\n",
    "                activated_transitions.add(step[1])\n",
    "    total_transitions = len(net.transitions)\n",
    "    support = len(activated_transitions) / total_transitions\n",
    "    \n",
    "    # Calculate Coverage (fraction of traces covered by at least one activated transition)\n",
    "    covered_traces = sum(1 for trace in aligned_traces if any(step[0] == step[1] and step[1] is not None for step in trace['alignment']))\n",
    "    total_traces = len(log)\n",
    "    coverage = covered_traces / total_traces\n",
    "\n",
    "    return support, coverage\n",
    "\n",
    "\n",
    "def evaluate_routines(log, net, im, fm, token_base, routine_label):\n",
    "    gt_routine_activities = extract_transitions(net)\n",
    "    routine_activities = set(log['concept:name'])\n",
    "\n",
    "    # case_id = log['case:concept:name'].unique()[0]\n",
    "    # routine_length = len(log[log['case:concept:name'] == case_id])\n",
    "    log_traces = len(log['case:concept:name'].unique())\n",
    "\n",
    "    # Calculate the Jaccard Coefficient (Intersection over Union)\n",
    "    intersection = routine_activities.intersection(gt_routine_activities)\n",
    "    union = routine_activities.union(gt_routine_activities)\n",
    "    jaccard_coefficient = len(intersection) / len(union)\n",
    "\n",
    "    log = pm4py.convert_to_event_log(log)\n",
    "    support, coverage = token_base_evaluation(log, net, im, fm) if token_base else alignment_base_evaluation(log, net, im, fm)\n",
    "\n",
    "    # Calculate fitness, precision, generalization, and F-score\n",
    "    q_o = algorithm.apply(log, net, im, fm)\n",
    "    fitness = round(q_o['fitness']['average_trace_fitness'],3)\n",
    "    prec = round(q_o['precision'],3)\n",
    "    gen = round(q_o['generalization'],3)\n",
    "    simp = round(q_o['simplicity'],3)\n",
    "    f_score = get_fScore(fitness, prec)\n",
    "\n",
    "    # Print metrics\n",
    "    # print(\"\\nEvaluation Scores:\")\n",
    "    # print(\"=====================\")\n",
    "    # print(\"Fitness: \", fitness)\n",
    "    # print(\"Precision: \", prec)\n",
    "    # print(\"Generalization: \", gen)\n",
    "    # print(\"Simplicity: \", simp)\n",
    "    \n",
    "    # print(f\"\\nCoverage: {coverage:.2f}\")\n",
    "    # print(f\"Support: {support:.2f}\")\n",
    "    # print(f\"Jaccard Coefficient: {jaccard_coefficient:.2f}\")\n",
    "\n",
    "    return [routine_label, log_traces, len(routine_activities), fitness, prec, gen, simp, f_score, coverage, support, jaccard_coefficient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d684f0-d6e1-4dda-a251-73c5c08c0861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3d564d-860e-417f-bb70-fea9b6b45310",
   "metadata": {},
   "source": [
    "# *Routine by Routine Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed5c50d-67b9-4ca9-b5ff-34de64a74f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UI_logs_evaluate(activity_log, cluster_map):\n",
    "    key_value = 1\n",
    "    for key, routine_label in cluster_map.items():\n",
    "        # print(f\"Start Evaluating the {routine_label}:\")\n",
    "        routine_log = activity_log[activity_log[cluster_column] == routine_label]\n",
    "        JC_scores = {}\n",
    "        for file_name in os.listdir(cpn_ground_truth_dir):\n",
    "            model_name = file_name.split('.')[0]\n",
    "            cpn_ground_truth_path = os.path.join(cpn_ground_truth_dir, file_name)\n",
    "            gt_net, gt_im, gt_fm = pnml_importer.import_net(cpn_ground_truth_path)\n",
    "            JC = get_JC(routine_log, gt_net, gt_im, gt_fm, routine_label=routine_label)\n",
    "            JC_scores[file_name] = JC\n",
    "            # print(f\"JC Score of {routine_label} is {JC} with {model_name}\")\n",
    "            \n",
    "        # Find the key with the maximum value\n",
    "        max_key = max(JC_scores, key=JC_scores.get)\n",
    "        max_value = JC_scores[max_key]\n",
    "    \n",
    "        # Output the result\n",
    "        # print(f\"\\nMaximum JC Score of {routine_label} is {max_value}\")\n",
    "    \n",
    "        cpn_ground_truth_path = os.path.join(cpn_ground_truth_dir, max_key)\n",
    "        gt_net, gt_im, gt_fm = pnml_importer.import_net(cpn_ground_truth_path)\n",
    "        scores = evaluate_routines(routine_log, gt_net, gt_im, gt_fm, token_base=False, routine_label=routine_label)\n",
    "        result_dic[key_value] = scores\n",
    "        key_value += 1\n",
    "        # print(\"\\n\\n\\n\")\n",
    "    return result_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626add2-d3ce-473a-8510-ecf1db5d8310",
   "metadata": {},
   "source": [
    "## *Summary of Evaluation Scores*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674e9d3c-19e8-4918-91f2-7fd023ceb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Function to calculate F-score given fitness and precision\n",
    "def calculate_f_score(fitness, precision):\n",
    "    if fitness + precision == 0:\n",
    "        return 0\n",
    "    return 2 * (fitness * precision) / (fitness + precision)\n",
    "\n",
    "# ✅ Function to calculate simple and weighted averages for a single result dictionary\n",
    "def UI_logs_summary(result_dic):\n",
    "    # Column names including Coverage, Support, and JC\n",
    "    columns = [\"Metrics\", \"Traces\", \"Trace Length\", \"Fitness\", \"Precision\", \n",
    "               \"Generalization\", \"Simplicity\", \"F-Score\", \"Coverage\", \"Support\", \"JC\"]\n",
    "\n",
    "    # Initialize lists for averages\n",
    "    simple_average_values = [\"Simple Average\"]\n",
    "    weighted_average_values = [\"Weighted Average\"]\n",
    "\n",
    "    total_traces = sum(result_dic[key][1] for key in result_dic if isinstance(key, int))\n",
    "    simple_average_values.append(total_traces)\n",
    "    weighted_average_values.append(total_traces)\n",
    "\n",
    "    mean_fitness = mean_precision = weighted_fitness = weighted_precision = 0\n",
    "\n",
    "    # Iterate through metrics starting from index 2 (\"Trace Length\")\n",
    "    for i in range(2, len(result_dic[1])):  \n",
    "        values = [result_dic[key][i] for key in result_dic if isinstance(key, int)]\n",
    "\n",
    "        simple_avg = sum(values) / len(values)\n",
    "        weighted_avg = sum(result_dic[key][i] * result_dic[key][1] for key in result_dic if isinstance(key, int)) / total_traces\n",
    "\n",
    "        simple_avg = round(simple_avg, 3)\n",
    "        weighted_avg = round(weighted_avg, 3)\n",
    "        \n",
    "        if i == 3:  # Fitness column\n",
    "            mean_fitness = simple_avg\n",
    "            weighted_fitness = weighted_avg\n",
    "        elif i == 4:  # Precision column\n",
    "            mean_precision = simple_avg\n",
    "            weighted_precision = weighted_avg\n",
    "\n",
    "        simple_average_values.append(simple_avg)\n",
    "        weighted_average_values.append(weighted_avg)\n",
    "\n",
    "    # 🔢 Calculate and update the F-Score\n",
    "    f_score_simple_avg = calculate_f_score(mean_fitness, mean_precision)\n",
    "    f_score_weighted_avg = calculate_f_score(weighted_fitness, weighted_precision)\n",
    "\n",
    "    # Update F-Score at the correct index (7th metric after 'Metrics' & 'Traces')\n",
    "    simple_average_values[7] = round(f_score_simple_avg, 3)\n",
    "    weighted_average_values[7] = round(f_score_weighted_avg, 3)\n",
    "\n",
    "    # ➕ Insert average rows into result_dic\n",
    "    next_key_simple = max([key for key in result_dic if isinstance(key, int)], default=0) + 1\n",
    "    next_key_weighted = next_key_simple + 1\n",
    "\n",
    "    result_dic[next_key_simple] = simple_average_values\n",
    "    result_dic[next_key_weighted] = weighted_average_values\n",
    "\n",
    "    # 📝 Convert to DataFrame and print\n",
    "    result_df = pd.DataFrame.from_dict(result_dic, orient='index', columns=columns)\n",
    "    result_df = result_df.set_index('Metrics')\n",
    "\n",
    "    # print(\"\\n✅ Per-Log Summary:\")\n",
    "    # print(tabulate(result_df, headers='keys', tablefmt='psql'))\n",
    "\n",
    "    return result_dic, simple_average_values  # Return simple average row for aggregation\n",
    "\n",
    "# 🚀 Function to handle cross-validation and final aggregation\n",
    "def cross_validation_summary(cross_val_results):\n",
    "    \"\"\"Performs cross-validation, collects simple averages, and calculates final averages.\"\"\"\n",
    "    iter_results = {}\n",
    "    simple_averages_list = []\n",
    "\n",
    "    # 📊 Collect simple averages from each iteration\n",
    "    for i, result_dic in enumerate(cross_val_results, start=1):\n",
    "        _, simple_avg_row = UI_logs_summary(result_dic)\n",
    "        iter_results[f\"CV {i}\"] = simple_avg_row[1:]  # Exclude 'Simple Average' label\n",
    "        simple_averages_list.append(simple_avg_row[1:])\n",
    "\n",
    "    # 🔎 Convert iteration results into DataFrame\n",
    "    iter_columns = [\"Traces\", \"Trace Length\", \"Fitness\", \"Precision\", \"Gen\", \n",
    "                    \"Simp\", \"F-Score\", \"Coverage\", \"Support\", \"JC\"]\n",
    "    iter_df = pd.DataFrame(iter_results, index=iter_columns).T\n",
    "\n",
    "    print(\"\\n✅ Cross-Validation Iteration Results:\")\n",
    "    print(tabulate(iter_df, headers='keys', tablefmt='psql'))\n",
    "\n",
    "    # 🧮 Final Simple and Weighted Averages\n",
    "    final_simple_avg = iter_df.mean().tolist()\n",
    "    total_traces = iter_df[\"Traces\"].sum()\n",
    "    weighted_avg = [(iter_df[col] * iter_df[\"Traces\"]).sum() / total_traces for col in iter_df.columns]\n",
    "\n",
    "    # 📢 Final Aggregated Results\n",
    "    final_df = pd.DataFrame(\n",
    "        [final_simple_avg, weighted_avg],\n",
    "        index=[\"Simple Average\", \"Weighted Average\"],\n",
    "        columns=iter_columns\n",
    "    )\n",
    "\n",
    "    print(\"\\n🚀 Final Aggregated Results:\")\n",
    "    print(tabulate(final_df, headers='keys', tablefmt='outline'))\n",
    "\n",
    "    return iter_df, final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666669e0-1577-4736-9455-397fe02a1b5e",
   "metadata": {},
   "source": [
    "## *main*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873d11b1-f262-4a4b-8b58-c52d7c05ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise = 0.4\n",
    "\n",
    "# for log_number in range(1, 10):\n",
    "#     if log_number==7:\n",
    "#         continue\n",
    "#     cross_val_results = []\n",
    "#     for variant in range(1, 11):\n",
    "#         cpn_ground_truth_dir = f\"GT_Models/log{log_number}/\"\n",
    "#         file_path = f\"Transformed_Logs_and_Results/Our/Transformed_Log_With_Noise_{Noise}/log{log_number}/noisy_transform_log{log_number}_{variant}.xes\"\n",
    "#         label_file_path = f\"Transformed_Logs_and_Results/arebmann/Transformed_Log_With_Noise_{Noise}/Discovered_Routines/log{log_number}/log{log_number}_{variant}_pred.csv\"\n",
    "\n",
    "#         result_dic = {'Metrics':['Routine', \"Traces\", \"Length\", 'Fitness','Precision','Generalization','Simplicity', 'F_Score', 'coverage', 'Support', 'JC'],}\n",
    "        \n",
    "#         event_log, cluster_labels_df = UI_read_log(file_path, label_file_path)\n",
    "#         event_log, cluster_labels_df = process_segments(event_log, cluster_labels_df)\n",
    "        \n",
    "#         session_log = f.create_session(event_log)\n",
    "#         encoded_log = f.freq_encoding(session_log)\n",
    "        \n",
    "#         cluster_column = \"cluster_label\"\n",
    "#         encoded_log = pd.merge(encoded_log, cluster_labels_df, on=['case:concept:name'], how='inner')\n",
    "#         encoded_log.rename(columns={'prediction': cluster_column}, inplace=True)\n",
    "        \n",
    "#         encoded_log = remove_noise_samples(encoded_log)\n",
    "#         activity_log, cluster_map = get_acitvity_log(session_log, encoded_log)\n",
    "        \n",
    "#         result_dic = UI_logs_evaluate(activity_log, cluster_map)\n",
    "        \n",
    "#         cross_val_results.append(result_dic)\n",
    "    \n",
    "#     # Run cross-validation summary\n",
    "#     iteration_results, final_averages = cross_validation_summary(cross_val_results)\n",
    "    \n",
    "#     results_df = pd.concat([iteration_results, final_averages])\n",
    "    \n",
    "#     outputfile = f\"Transformed_Logs_and_Results/arebmann/Transformed_Log_With_Noise_{Noise}/Results\"\n",
    "#     if not os.path.exists(outputfile):\n",
    "#         os.makedirs(outputfile)\n",
    "#     output_file_name = f\"log{log_number}_reuslts.csv\"\n",
    "#     results_df.to_csv(os.path.join(outputfile, output_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809e5b5-b487-47fa-a428-7af4a67b08b5",
   "metadata": {},
   "source": [
    "## *For Noise Free Logs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db22d36-0e4e-4cf3-89a6-77d6ab00340f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|████████████████████████████████████████████| 1500/1500 [00:05<00:00, 284.32it/s]\n",
      "aligning log, completed variants ::   0%|                                                        | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "Noise = 0\n",
    "\n",
    "for log_number in range(7, 8):\n",
    "    cpn_ground_truth_dir = f\"GT_Models/log{log_number}/\"\n",
    "    file_path = f\"Transformed_Logs_and_Results/Our/Transformed_Log_Without_Noise/transform_log{log_number}.xes\"\n",
    "    label_file_path = f\"Transformed_Logs_and_Results/arebmann/Transformed_Log_With_Noise_{Noise}/Discovered_Routines/log{log_number}/log{log_number}_1_pred.csv\"\n",
    "\n",
    "    result_dic = {'Metrics':['Routine', \"Traces\", \"Length\", 'Fitness','Precision','Generalization','Simplicity', 'F_Score', 'coverage', 'Support', 'JC'],}\n",
    "    \n",
    "    event_log, cluster_labels_df = UI_read_log(file_path, label_file_path)\n",
    "    event_log, cluster_labels_df = process_segments(event_log, cluster_labels_df)\n",
    "    \n",
    "    session_log = f.create_session(event_log)\n",
    "    encoded_log = f.freq_encoding(session_log)\n",
    "    \n",
    "    cluster_column = \"cluster_label\"\n",
    "    encoded_log = pd.merge(encoded_log, cluster_labels_df, on=['case:concept:name'], how='inner')\n",
    "    encoded_log.rename(columns={'prediction': cluster_column}, inplace=True)\n",
    "    \n",
    "    encoded_log = remove_noise_samples(encoded_log)\n",
    "    activity_log, cluster_map = get_acitvity_log(session_log, encoded_log)\n",
    "    \n",
    "    result_dic = UI_logs_evaluate(activity_log, cluster_map)\n",
    "    result_dic, simple_average_values = UI_logs_summary(result_dic)\n",
    "\n",
    "    result_df = pd.DataFrame(result_dic)\n",
    "    result_df = result_df.set_index('Metrics')\n",
    "    results_df = result_df.T\n",
    "    \n",
    "    outputfile = f\"Transformed_Logs_and_Results/arebmann/Transformed_Log_With_Noise_{Noise}/Results\"\n",
    "    if not os.path.exists(outputfile):\n",
    "        os.makedirs(outputfile)\n",
    "    output_file_name = f\"log{log_number}_reuslts.csv\"\n",
    "    results_df.to_csv(os.path.join(outputfile, output_file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292db7a-f5c4-445e-91fe-836b7b9f260d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
